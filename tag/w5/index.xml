<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>w5 | Seminar in Cognitive Modelling</title>
    <link>https://mkunda.github.io/scm24/tag/w5/</link>
      <atom:link href="https://mkunda.github.io/scm24/tag/w5/index.xml" rel="self" type="application/rss+xml" />
    <description>w5</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jul 2023 09:58:49 +0100</lastBuildDate>
    <image>
      <url>https://mkunda.github.io/scm24/media/icon_hu9107f20a553ba23fec53c69f31ee61df_369781_512x512_fill_lanczos_center_3.png</url>
      <title>w5</title>
      <link>https://mkunda.github.io/scm24/tag/w5/</link>
    </image>
    
    <item>
      <title>Objects &amp; Events</title>
      <link>https://mkunda.github.io/scm24/projects/objects/</link>
      <pubDate>Sat, 01 Jul 2023 09:58:49 +0100</pubDate>
      <guid>https://mkunda.github.io/scm24/projects/objects/</guid>
      <description>&lt;h2 id=&#34;objects&#34;&gt;Objects&lt;/h2&gt;
&lt;p&gt;Light hits our eyes  and somehow we perceive 3 dimensional &amp;lsquo;objects&amp;rsquo;.
What is an object?
In representational systems like programming languages, objects are core structure.
Is the same true for our cognitive system? Are objects learned or innate?&lt;/p&gt;
&lt;p&gt;Similar to object, events seem like a great candidate for an ontological type. Formal semanticists have illustrated time and time again that they are important for explaining language. But are events core conceptual knowledge?&lt;/p&gt;
&lt;h2 id=&#34;events&#34;&gt;Events&lt;/h2&gt;
&lt;p&gt;Similar to object, events seem like a great candidate for an ontological type. Formal semanticists have illustrated time and time again that they are important for explaining language. But are events core conceptual knowledge?&lt;/p&gt;
&lt;h2 id=&#34;primary-readings&#34;&gt;Primary Readings&lt;/h2&gt;
&lt;p&gt;Everyone should read these and be prepared to discuss.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spelke, E.S. (1990) Principles of object perception. &lt;em&gt;Cognitive Science&lt;/em&gt;, 14(1), 29-56.&lt;/li&gt;
&lt;li&gt;Zacks, J. M., &amp;amp; Tversky, B. (2001) Event structure in perception and conception. &lt;em&gt;Psychological Bulletin&lt;/em&gt;, 127(1), 3.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Inductive Reasoning</title>
      <link>https://mkunda.github.io/scm24/projects/induction/</link>
      <pubDate>Sat, 01 Jul 2023 09:58:49 +0100</pubDate>
      <guid>https://mkunda.github.io/scm24/projects/induction/</guid>
      <description>&lt;p&gt;While Sherlock&amp;rsquo;s over there blogging &lt;em&gt;The Science of Deduction&lt;/em&gt;, I&amp;rsquo;d argue most of human reasoning is &lt;strong&gt;inductive&lt;/strong&gt;. We see lot&amp;rsquo;s of examples (e.g., 10 million white swans) and then try to explain them (swans are white). Checks out, right?&lt;/p&gt;
&lt;h2 id=&#34;primary-readings&#34;&gt;Primary Readings&lt;/h2&gt;
&lt;p&gt;Everyone should read these and be prepared to discuss:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;
  &lt;i class=&#34;fas fa-scroll  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Goodman, N. (1955)&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;details&gt;
  &lt;summary&gt;The Riddle of Induction. Chapter 3 of &lt;em&gt;Fact, fiction, and forecast&lt;/em&gt; (pp. 59&amp;ndash;83). Cambridge, MA: Harvard University Press. (On Learn)&lt;/summary&gt;
  &lt;i class=&#34;fas fa-chalkboard-teacher  pr-1 fa-fw&#34;&gt;&lt;/i&gt;This is a classic. Nelson Goodman  was an influential philosopher of science (not to be confused with the living Standford based scientist Noah Goodman below). His thought experiment about Grue, in particular has been a persistant challenge for theories of induction.
&lt;/details&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;
  &lt;i class=&#34;fas fa-scroll  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Wason, P. C. (1960).&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;details&gt;
  &lt;summary&gt;On the failure to eliminate hypotheses in a conceptual task. &lt;em&gt;Quarterly Journal of Experimental Psychology&lt;/em&gt;, 12(3), 129-140.&lt;/summary&gt;
  &lt;i class=&#34;fas fa-chalkboard-teacher  pr-1 fa-fw&#34;&gt;&lt;/i&gt;This is another classic. Peter Wason who we met in Representation topic has had at least two lasting impacts on cogntibve science: One with his card selection task analysis and another with this inductive reasoning task.
&lt;/details&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- 
  &lt;i class=&#34;fas fa-robot  pr-1 fa-fw&#34;&gt;&lt;/i&gt;  The article explores how the mind develops through the use of statistics, structure, and abstraction. It emphasizes the importance of understanding how the mind processes information and how it can be applied to various fields such as artificial intelligence and cognitive science. It focuses on the challenges of learning from sparse, noisy, and ambiguous data and highlights the ability of children to learn new words and concepts from just a few examples. It explains how Bayesian principles in the human mind are applied to specific cognitive capacities and modules. The article discusses the importance of abstract knowledge in learning, how learners acquire this knowledge, and different forms of abstract knowledge representation used in Bayesian cognitive models. It also discusses the concept of hierarchical Bayesian models and their role in learning abstract knowledge. The article concludes by highlighting the potential of Bayesian approaches in understanding cognition and its origins. --&gt;
&lt;h2 id=&#34;secondary-readings&#34;&gt;Secondary Readings&lt;/h2&gt;
&lt;p&gt;The presenter should read and incorporate these:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;
  &lt;i class=&#34;fas fa-scroll  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Tenenbaum, J. B., Kemp, C., Griffiths, T. L., &amp;amp; Goodman, N. D. (2011).&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;details&gt;
  &lt;summary&gt;How to grow a mind: Statistics, structure, and abstraction. &lt;em&gt;Science&lt;/em&gt;, 331(6022), 1279-1285.&lt;/summary&gt;
  &lt;p&gt;&lt;i class=&#34;fas fa-chalkboard-teacher  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Here&amp;rsquo;s another relatively modern paper as a comparison. Again it is interesting to compare the writing styles. It would seem like hierarchical Bayesian inference can help make more sense of how our inductive reasoning works, but does it really solve the riddle of induction? Note this paper is also quite relevant to the Rationality topic.&lt;/p&gt;
&lt;p&gt;&lt;i class=&#34;fas fa-robot  pr-1 fa-fw&#34;&gt;&lt;/i&gt;  The article explores how the mind develops through the use of statistics, structure, and abstraction. It emphasizes the importance of understanding how the mind processes information and how it can be applied to various fields such as artificial intelligence and cognitive science. It focuses on the challenges of learning from sparse, noisy, and ambiguous data and highlights the ability of children to learn new words and concepts from just a few examples. It explains how Bayesian principles in the human mind are applied to specific cognitive capacities and modules. The article discusses the importance of abstract knowledge in learning, how learners acquire this knowledge, and different forms of abstract knowledge representation used in Bayesian cognitive models. It also discusses the concept of hierarchical Bayesian models and their role in learning abstract knowledge. The article concludes by highlighting the potential of Bayesian approaches in understanding cognition and its origins.&lt;/p&gt;

&lt;/details&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;
  &lt;i class=&#34;fas fa-scroll  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Goyal, A., &amp;amp; Bengio, Y. (2022).&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;details&gt;
  &lt;summary&gt;Inductive biases for deep learning of higher-level cognition. &lt;em&gt;Proceedings of the Royal Society A&lt;/em&gt;, 478 (20210068).&lt;/summary&gt;
  &lt;p&gt;&lt;i class=&#34;fas fa-robot  pr-1 fa-fw&#34;&gt;&lt;/i&gt;  This paper examines the hypothesis that human and animal intelligence can be explained by a few principles. It focuses on the inductive biases used in deep learning for higher-level and sequential conscious processing in order to close the gap between current deep learning and human cognitive abilities. It emphasizes the need for additional inductive biases to achieve flexibility, robustness, and adaptability in deep learning models, particularly through knowledge representation. It also discusses the limitations of current machine learning systems in terms of their performance, generalizability, and robustness.&lt;/p&gt;
&lt;p&gt;&lt;i class=&#34;fas fa-chalkboard-teacher  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Here&amp;rsquo;sa  recent paper by some of the big names in machine learning. It highlights very similar issues and solutions to problems of inductive generalization and the role of priors or inductive biases as arise in the human cognition literature.&lt;/p&gt;

&lt;/details&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;questions-under-discussion&#34;&gt;Questions under discussion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How/do humans rationally solve the problem of induction?&lt;/li&gt;
&lt;li&gt;What are inductive biases and what are they good for?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
